{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import re\n",
    "import os\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from crosspy.core.methods import cplv, cplv_pairwise\n",
    "from crosspy.preprocessing.seeg.support import clean_montage, drop_monopolar_channels\n",
    "from crosspy.preprocessing.seeg.seeg_utils import create_reference_mask, get_electrode_distance\n",
    "from crosspy.preprocessing.signal import preprocess_data_morlet\n",
    "\n",
    "from bids import BIDSLayout\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "import pickle\n",
    "\n",
    "import tqdm\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import scipy.io\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import itertools\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "cdict1 = {'red':   ((0.0, 0.0, 0.0),\n",
    "                   (0.166, 0.43, 0.43),\n",
    "                   (0.33, 0.7, 0.7),\n",
    "                   (0.5, 1.0, 1.0),\n",
    "                   (0.66, 0.8, 0.8),\n",
    "                   (1.0, 0.6, 0.6)),\n",
    "\n",
    "         'green': ((0.0, 0.4, 0.4),\n",
    "                   (0.166, 0.7, 0.7),\n",
    "                   (0.33, 0.8, 0.8),\n",
    "                   (0.5, 1.0, 1.0),\n",
    "                   (0.66, 0.8, 0.8),\n",
    "                   (1.0,0.0, 0.0)),\n",
    "\n",
    "         'blue':  ((0.0, 0.8, 0.8),\n",
    "                   (0.166, 1.0, 1.0),\n",
    "                   (0.33, 1.0, 1.0),\n",
    "                   (0.5, 0.4, 0.4),\n",
    "                   (0.66, 0.0, 0.0),\n",
    "                   (1.0, 0.0, 0.0))\n",
    "        }\n",
    "\n",
    "ripples_cmap = LinearSegmentedColormap('ripplescmap', cdict1)\n",
    "ripples_blue = ripples_cmap(0)\n",
    "ripples_red = ripples_cmap(0.99)\n",
    "ripples_orange = ripples_cmap(0.7)\n",
    "\n",
    "colors = [ripples_cmap(0), ripples_cmap(0.33), ripples_cmap(0.66), ripples_cmap(0.99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../seeg_phases/data/SEEG_redux_BIDS/convert_results.csv', sep=',')\n",
    "df = pd.read_csv('/home/gabri/localdata/convert_results.csv', sep=';', skiprows=1)\n",
    "df_num_idx = df.set_index('subject_number')\n",
    "\n",
    "subject_to_cluster = pd.read_csv('/home/gabri/localdata/Subject_agg_cluster_labels.csv').set_index('subject_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ez_mask(ez_chans):\n",
    "    res = np.zeros((ez_chans.shape[0], ez_chans.shape[0]), dtype=bool)\n",
    "    for i in range(ez_chans.shape[0]):\n",
    "        for j in range(ez_chans.shape[0]):\n",
    "            res[i,j] = ez_chans[i] == 0 and ez_chans[j] == 0\n",
    "    \n",
    "    return res\n",
    "\n",
    "def create_gmpi_masks(ch_names, meta):\n",
    "    def _get_type(idx):\n",
    "        if -0.3 < ch_gmpi[idx] <= 0:\n",
    "            return 1\n",
    "        elif 0.5 < ch_gmpi[idx] < 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    n_chans = len(ch_names)\n",
    "    ch_gmpi = [meta.loc[ch]['GMPI'] for ch in ch_names]\n",
    "    \n",
    "    ss_mask = np.zeros((n_chans, n_chans), dtype=bool)\n",
    "    dd_mask = np.zeros((n_chans, n_chans), dtype=bool)\n",
    "    \n",
    "    for i, j in itertools.product(range(n_chans), range(n_chans)):\n",
    "        i_type = _get_type(i)\n",
    "        j_type = _get_type(j)\n",
    "        \n",
    "        if i_type == 1 and j_type == 1:\n",
    "            dd_mask[i,j] = True\n",
    "        elif i_type == 0 and j_type == 0:\n",
    "            ss_mask[i,j] = True\n",
    "    \n",
    "    return dd_mask, ss_mask\n",
    "\n",
    "def create_gmpi_masks_2(gmpi, shuffle=False):\n",
    "    \n",
    "    gmpi[np.isnan(gmpi)] = -200.\n",
    "\n",
    "    dd_ = np.logical_and(-.3 < gmpi, gmpi < 0)\n",
    "    ss_ = np.logical_and(.5 < gmpi, gmpi < 1)\n",
    "    \n",
    "    if shuffle:\n",
    "        dd_indices = np.nonzero(dd_)[0]\n",
    "        ss_indices = np.nonzero(ss_)[0]\n",
    "        n_dd = dd_indices.shape[0]\n",
    "        n_ss = ss_indices.shape[0]\n",
    "        edge_indices = np.concatenate((dd_indices,ss_indices))\n",
    "        np.random.shuffle(edge_indices)\n",
    "        dd_mix = edge_indices[:n_dd]\n",
    "        ss_mix = edge_indices[n_dd:]\n",
    "        dd_ = np.zeros((dd_.shape),dtype=int)\n",
    "        ss_ = np.zeros((ss_.shape),dtype=int)\n",
    "\n",
    "        dd_[dd_mix] = 1\n",
    "        ss_[ss_mix] = 1\n",
    "        \n",
    "    dd_mask = np.dot(dd_[:,np.newaxis],dd_[:,np.newaxis].T)\n",
    "    ss_mask = np.dot(ss_[:,np.newaxis],ss_[:,np.newaxis].T)\n",
    "    \n",
    "    return dd_mask, ss_mask\n",
    "\n",
    "def get_spectrum_by_bins(data, filter_mask, distance_matrix):\n",
    "    spectrum = data['cplv_spectrum']\n",
    "    surrogate_spectrum = data['surrogate_spectrum']\n",
    "    \n",
    "    n_bins = len(distance_bins) - 1\n",
    "    dists_binned = np.digitize(distance_matrix, bins=distance_bins) - 1\n",
    "    \n",
    "    plv_binned = np.zeros((n_bins, spectrum.shape[0]))\n",
    "    sur_plv_binned = np.zeros((n_bins, spectrum.shape[0]))\n",
    "    iplv_binned = np.zeros((n_bins, spectrum.shape[0]))\n",
    "    sur_iplv_binned = np.zeros((n_bins, spectrum.shape[0]))\n",
    "    plv_k = np.zeros((n_bins, spectrum.shape[0]))\n",
    "    iplv_k = np.zeros((n_bins, spectrum.shape[0]))\n",
    "    \n",
    "    for bin_idx in range(n_bins):\n",
    "        bin_mask = (dists_binned == bin_idx)\n",
    "        \n",
    "        mask = np.triu(filter_mask & bin_mask, 1)\n",
    "\n",
    "        if np.any(mask > 0):\n",
    "            plv_masked = np.abs(spectrum[:, mask])\n",
    "            iplv_masked = np.abs(np.imag(spectrum[:, mask]))\n",
    "\n",
    "            plv_surr_values = np.abs(surrogate_spectrum[:,mask]).mean(axis=1, keepdims=True)*3.52\n",
    "            iplv_surr_values = np.abs(np.imag(surrogate_spectrum[:,mask])).mean(axis=1, keepdims=True)*3.52\n",
    "\n",
    "            plv_binned[bin_idx] = plv_masked.mean(axis=1)\n",
    "            iplv_binned[bin_idx] = iplv_masked.mean(axis=1)\n",
    "            sur_plv_binned[bin_idx] = np.abs(surrogate_spectrum[:,mask]).mean(axis=1)\n",
    "            sur_iplv_binned[bin_idx] = np.abs(np.imag(surrogate_spectrum[:,mask])).mean(axis=1)\n",
    "            \n",
    "            plv_k[bin_idx] = np.mean(plv_masked >= plv_surr_values, axis=1)\n",
    "            iplv_k[bin_idx] = np.mean(iplv_masked >= iplv_surr_values, axis=1)\n",
    "    \n",
    "    return plv_binned, iplv_binned, plv_k, iplv_k, sur_plv_binned, sur_iplv_binned\n",
    "    \n",
    "def is_bipolar(x):\n",
    "    a, c = x.split('-')\n",
    "    \n",
    "    return len(c) > 0\n",
    "\n",
    "def get_frequencies():\n",
    "    return np.load('gabriele/frequencies.npy')\n",
    "\n",
    "def _get_meta(path_subject, file_type):\n",
    "    return glob.glob(path_subject+'/*'+file_type+'*')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_bins = [0,  32,  45,  60, 137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actual routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.join('/home/gabri/localdata/rest-bids/')\n",
    "layout = BIDSLayout('/home/gabri/localdata/rest-bids/')\n",
    "\n",
    "subject_numbers = list()\n",
    "\n",
    "n_rep = 1000\n",
    "# 2 layers, 4 distance ranges, 50 frequencies and 68 subjects\n",
    "plv_layers = np.zeros((2, 4, 50, 68))\n",
    "plv_layers_sur = np.zeros((2, 4, 50, 68))\n",
    "plv_layers_k = np.zeros((2, 4, 50, 68))\n",
    "plv_layers_stats = np.zeros((2, 4, 50, 68,n_rep))\n",
    "\n",
    "iplv_layers = np.zeros((2, 4, 50, 68))\n",
    "iplv_layers_sur = np.zeros((2, 4, 50, 68))\n",
    "iplv_layers_k = np.zeros((2, 4, 50, 68))\n",
    "iplv_layers_stats = np.zeros((2, 4, 50, 68,n_rep))\n",
    "\n",
    "for subject in tqdm.tqdm_notebook(layout.get(target='subject', extension='edf', task='rest')): \n",
    "    subj_num = subject.entities['subject']\n",
    "    subj_name = df_num_idx.loc[int(subj_num)]['name']\n",
    "    subj_idx = int(subj_num)-1\n",
    "    \n",
    "    meta_fname  = '/home/gabri/localdata/rest-bids/sub-{}/ses-01/ieeg/sub-{}_meta.csv'.format(subj_num, subj_num)\n",
    "    res_fname = '/home/gabri/localdata/rest-bids/derivatives/crosspy/plv_spectrum_cw_wake/sub-{}_spectrum_with_lp.pickle'.format(subject.entities['subject'])\n",
    "\n",
    "    if subj_num in {'36', '57', '18'} or subject_to_cluster.loc[int(subj_num)]['subject_cluster'] == 4:\n",
    "        print('Excluding {} from analysis'.format(subj_idx))\n",
    "        continue\n",
    "    \n",
    "    if not(os.path.exists(meta_fname)):\n",
    "        print('Subj {} does not have meta!'.format(subj_idx))\n",
    "        continue\n",
    "        \n",
    "    subject_numbers.append(subj_idx-1)\n",
    "    \n",
    "    subj_meta = pd.read_csv(meta_fname).set_index('contact')\n",
    "\n",
    "    with open(res_fname, 'rb') as f:\n",
    "        res_data = pickle.load(f)\n",
    "\n",
    "    ez_tags = subj_meta.EZ.to_numpy()\n",
    "    nez_tags = (ez_tags  == 0).astype(int)\n",
    "    nez_mask = np.dot(nez_tags[:,np.newaxis],nez_tags[:,np.newaxis].T)\n",
    "    nez_mask = nez_mask.astype(bool)\n",
    "    nez_mask = np.ones(nez_mask.shape, dtype=bool)\n",
    "\n",
    "    ref_mask = res_data['reference_mask'].astype(bool)\n",
    "    frequencies = res_data['frequencies']  \n",
    "    distance_matrix = res_data['electrodes_distance']\n",
    "    \n",
    "    gmpi = subj_meta.GMPI.to_numpy()\n",
    "    dd_mask, ss_mask = create_gmpi_masks_2(gmpi,shuffle=False)\n",
    "    \n",
    "    dd_mask = np.triu(np.logical_and.reduce([ref_mask, nez_mask, dd_mask]), 1)\n",
    "    ss_mask = np.triu(np.logical_and.reduce([ref_mask, nez_mask, ss_mask]), 1)\n",
    "    \n",
    "    dd_out = get_spectrum_by_bins(res_data, dd_mask, distance_matrix)\n",
    "    ss_out = get_spectrum_by_bins(res_data, ss_mask, distance_matrix)\n",
    "    \n",
    "    plv_layers[0,:,:,subj_idx] = dd_out[0]\n",
    "    plv_layers[1,:,:,subj_idx] = ss_out[0]\n",
    "    \n",
    "    iplv_layers[0,:,:,subj_idx] = dd_out[1]\n",
    "    iplv_layers[1,:,:,subj_idx] = ss_out[1]\n",
    "    \n",
    "    plv_layers_k[0,:,:,subj_idx] = dd_out[2]\n",
    "    plv_layers_k[1,:,:,subj_idx] = ss_out[2]\n",
    "    \n",
    "    iplv_layers_k[0,:,:,subj_idx] = dd_out[3]\n",
    "    iplv_layers_k[1,:,:,subj_idx] = ss_out[3]\n",
    "    \n",
    "    plv_layers_sur[0,:,:,subj_idx] = dd_out[4]\n",
    "    plv_layers_sur[1,:,:,subj_idx] = ss_out[4]\n",
    "    \n",
    "    iplv_layers_sur[0,:,:,subj_idx] = dd_out[5]\n",
    "    iplv_layers_sur[1,:,:,subj_idx] = ss_out[5]\n",
    "\n",
    "    for rep_idx in range(n_rep):\n",
    "        \n",
    "        dd_mask, ss_mask = create_gmpi_masks_2(gmpi,shuffle=True)\n",
    "        dd_mask = np.triu(np.logical_and.reduce([ref_mask, nez_mask, dd_mask]), 1)\n",
    "        ss_mask = np.triu(np.logical_and.reduce([ref_mask, nez_mask, ss_mask]), 1)\n",
    "        \n",
    "        subj_dd_plv_binned,subj_dd_iplv_binned,subj_dd_k_plv_binned,subj_dd_k_iplv_binned,_,_ = get_spectrum_by_bins(res_data, dd_mask, distance_matrix)\n",
    "        subj_ss_plv_binned, subj_ss_iplv_binned, _, _,_,_ = get_spectrum_by_bins(res_data, ss_mask, distance_matrix)\n",
    "        plv_layers_stats[0,:,:,subj_idx,rep_idx] = subj_dd_plv_binned\n",
    "        plv_layers_stats[1,:,:,subj_idx,rep_idx] = subj_ss_plv_binned\n",
    "        iplv_layers_stats[0,:,:,subj_idx,rep_idx] = subj_dd_iplv_binned\n",
    "        iplv_layers_stats[1,:,:,subj_idx,rep_idx] = subj_ss_iplv_binned\n",
    "        \n",
    "# remove from resulting matrix the empty subjects\n",
    "empty_subj = np.nonzero(plv_layers[0,1,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plv_layers = np.squeeze(plv_layers[:,:,:,empty_subj])\n",
    "plv_layers_k = np.squeeze(plv_layers_k[:,:,:,empty_subj])\n",
    "plv_layers_stats = np.squeeze(plv_layers_stats[:,:,:,empty_subj,:])\n",
    "\n",
    "iplv_layers = np.squeeze(iplv_layers[:,:,:,empty_subj])\n",
    "iplv_layers_k = np.squeeze(iplv_layers_k[:,:,:,empty_subj])\n",
    "iplv_layers_stats = np.squeeze(iplv_layers_stats[:,:,:,empty_subj,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max T stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_T_stats(group_stats, alpha=.05):\n",
    "    # compute statistics -> absolute diff between groups i.e. layers\n",
    "    t_stat = np.mean(np.abs(np.diff(group_stats, axis=0)),axis=3)\n",
    "    \n",
    "    # get max t_stat along frequency axis\n",
    "    max_t_stat = np.max(t_stat, axis=2)\n",
    "\n",
    "    # get percentiles for one tail test since we're using abs diff\n",
    "    ci = np.percentile(max_t_stat, (1-alpha)*100, axis=2)\n",
    "\n",
    "    return np.squeeze(ci)\n",
    "\n",
    "T_obs = np.squeeze(np.mean(np.abs(np.diff(plv_layers, axis=0)),axis=(3)))\n",
    "max_T = get_max_T_stats(plv_layers_stats, alpha=0.05)\n",
    "\n",
    "titles = ['vSH', 'Short','Mid','long']\n",
    "_, axs = plt.subplots(1,4, figsize=(18,5), sharey=True)\n",
    "for idx,ax in enumerate(axs):\n",
    "    #ax.semilogx(frequencies, np.squeeze(np.abs(np.diff(plv_layers[:,idx,:,:], axis=0))),'grey')\n",
    "    ax.semilogx(frequencies, T_obs[idx,:], 'k', linewidth=2)\n",
    "    ax.semilogx(frequencies, np.tile(max_T[idx],50), 'k--')\n",
    "    ax.set_xlabel('Freq. [Hz]')\n",
    "    ax.set_title(titles[idx])\n",
    "    ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "\n",
    "axs[0].set_ylabel('|diff(ss,dd)|')\n",
    "axs[0].legend(('layer diff','p<.05 _ max T'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B-H FDR correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pvalue(group_stats, T_obs):\n",
    "    # compute statistics -> absolute diff between groups i.e. layers\n",
    "    t_stat = np.squeeze(np.mean(np.abs(np.diff(group_stats, axis=0)),axis=3))\n",
    "    \n",
    "    #compute fraction of T_obs > than T_surr\n",
    "    pval = np.mean(t_stat >= np.tile(T_obs[:,:,np.newaxis],t_stat.shape[-1:]) , axis=2)\n",
    "    return pval\n",
    "\n",
    "def bh_correction(pval, alpha=0.05):\n",
    "    pval_sort_idx = np.argsort(pval)\n",
    "    pval_sorted = pval[pval_sort_idx]\n",
    "    p_ranks = np.arange(1,51)\n",
    "    p_tmp = (p_ranks/50)*alpha\n",
    "    h = np.zeros((50,),dtype=bool)\n",
    "    h[:np.max(np.nonzero(pval_sorted <= p_tmp)[0])] = True\n",
    "    return h[pval_sort_idx]\n",
    "    \n",
    "# T_obs = np.squeeze(np.mean(np.abs(np.diff(iplv_layers, axis=0)),axis=(3)))\n",
    "iplv_layer_diff = np.abs(np.diff(iplv_layers, axis=0).squeeze())\n",
    "plv_layer_diff = np.abs(np.diff(plv_layers, axis=0).squeeze())\n",
    "\n",
    "T_obs = np.mean(np.abs(iplv_layer_diff),axis=(2))\n",
    "pval = get_pvalue(iplv_layers_stats, T_obs)\n",
    "\n",
    "titles = ['vSH', 'Short','Mid','long']\n",
    "_, axs = plt.subplots(1,4, figsize=(18,5), sharey=True)\n",
    "for idx,ax in enumerate(axs):\n",
    "    ax.semilogx(frequencies, T_obs[idx,:], 'yo', linewidth=2)\n",
    "    h = multipletests(pval[idx,:],alpha=.05, method='fdr_bh')[0]\n",
    "    ax.semilogx(frequencies[h],T_obs[idx,h], 'ko')\n",
    "    ax.set_xlabel('Freq. [Hz]')\n",
    "    ax.set_title(titles[idx])\n",
    "    ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "\n",
    "axs[0].set_ylabel('|diff(ss,dd)|')\n",
    "axs[0].legend(('layer diff','p<.05 _ max T'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = np.load('cov_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplv_diff_corr = np.zeros((100, 4))\n",
    "plv_diff_corr = np.zeros((100, 4))\n",
    "\n",
    "for sidx, s in enumerate(splits):\n",
    "    cohort_1 = s[:34] + 1\n",
    "    cohort_2 = s[34:] + 1\n",
    "\n",
    "    cohort_1 = [s for s in cohort_1 if s != 36 and subject_to_cluster.loc[s]['subject_cluster'] != 4]\n",
    "    cohort_2 = [s for s in cohort_2 if s != 36 and subject_to_cluster.loc[s]['subject_cluster'] != 4]\n",
    "\n",
    "    cohort_1_mask = np.in1d(subject_numbers, cohort_1)\n",
    "    cohort_2_mask = np.in1d(subject_numbers, cohort_2)\n",
    "    \n",
    "    c1_iplv = iplv_layer_diff[..., cohort_1_mask].mean(axis=-1)\n",
    "    c2_iplv = iplv_layer_diff[..., cohort_2_mask].mean(axis=-1)\n",
    "    \n",
    "    c1_plv = plv_layer_diff[..., cohort_1_mask].mean(axis=-1)\n",
    "    c2_plv = plv_layer_diff[..., cohort_2_mask].mean(axis=-1)\n",
    "    \n",
    "    iplv_diff_corr[sidx] = np.diag(np.corrcoef(c1_iplv, c2_iplv), 4)\n",
    "    plv_diff_corr[sidx] = np.diag(np.corrcoef(c1_plv, c2_plv), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplv_diff_corr_surr = np.zeros((100, 4))\n",
    "plv_diff_corr_surr = np.zeros((100, 4))\n",
    "\n",
    "for sidx, s in enumerate(splits):\n",
    "    s = s.copy()\n",
    "    np.random.shuffle(s)\n",
    "    \n",
    "    cohort_1 = s[:34] + 1\n",
    "    cohort_2 = s[34:] + 1\n",
    "\n",
    "    cohort_1 = [s for s in cohort_1 if s != 36 and subject_to_cluster.loc[s]['subject_cluster'] != 4]\n",
    "    cohort_2 = [s for s in cohort_2 if s != 36 and subject_to_cluster.loc[s]['subject_cluster'] != 4]\n",
    "\n",
    "    cohort_1_mask = np.in1d(subject_numbers, cohort_1)\n",
    "    cohort_2_mask = np.in1d(subject_numbers, cohort_2)\n",
    "    \n",
    "    c1_iplv = iplv_layer_diff[..., cohort_1_mask].mean(axis=-1)\n",
    "    c2_iplv = iplv_layer_diff[..., cohort_2_mask].mean(axis=-1)\n",
    "    \n",
    "    c1_plv = plv_layer_diff[..., cohort_1_mask].mean(axis=-1)\n",
    "    c2_plv = plv_layer_diff[..., cohort_2_mask].mean(axis=-1)\n",
    "    \n",
    "    iplv_diff_corr_surr[sidx] = np.diag(np.corrcoef(c1_iplv, c2_iplv), 4)\n",
    "    plv_diff_corr_surr[sidx] = np.diag(np.corrcoef(c1_plv, c2_plv), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_1_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10, 12.5), nrows=2)\n",
    "\n",
    "# _ = axes[0].boxplot(plv_coefficients)\n",
    "sns.boxplot(data=plv_diff_corr, ax=axes[0])\n",
    "sns.swarmplot(data=plv_diff_corr, ax=axes[0], palette=colors)\n",
    "\n",
    "sns.boxplot(data=iplv_diff_corr, ax=axes[1])\n",
    "sns.swarmplot(data=iplv_diff_corr, ax=axes[1], palette=colors)\n",
    "\n",
    "for box in axes[0].artists + axes[1].artists:\n",
    "    box.set_facecolor('white')\n",
    "    \n",
    "axes[0].set_title('PLV layer difference', fontsize=24)\n",
    "axes[1].set_title('iPLV layer difference', fontsize=24)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(['Very short', 'Short', 'Medium', 'Long'])\n",
    "    ax.tick_params(labelsize=18)\n",
    "    ax.set_ylabel('Correlation', fontsize=20)\n",
    "    ax.set_ylim([0.4, 1])\n",
    "    \n",
    "fig.savefig('split_layer_diff_correlation.svg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mean_with_bootstrap(spectrum, N_rounds=1000):\n",
    "    all_indices = np.arange(spectrum.shape[0])\n",
    "        \n",
    "    res = np.zeros((N_rounds, ) + spectrum.shape[1:])\n",
    "    for idx in range(N_rounds):\n",
    "        round_indices = np.random.choice(all_indices, size=all_indices.shape[0])\n",
    "        res[idx] = spectrum[round_indices].mean(axis=0)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(data, n=100, axis=0, percentiles=(2.5,97.5)):\n",
    "   \n",
    "    bootstraps = np.ndarray((n,data.shape[0]), dtype = data.dtype)\n",
    "\n",
    "    for idx in range(n):\n",
    "        perm_indices = np.random.randint(0,data.shape[1], data.shape[1])\n",
    "        bootstraps[idx,:] = data[:,perm_indices].mean(axis=1)\n",
    "    \n",
    "    ci = np.squeeze(np.percentile(bootstraps, percentiles, keepdims=True, axis=0))\n",
    "\n",
    "    return ci[0,:], ci[1,:]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(25, 18), ncols=4, nrows=4, sharey='row')\n",
    "\n",
    "group_plv_layers = np.mean(plv_layers,axis=3)\n",
    "group_iplv_layers = np.mean(iplv_layers,axis=3)\n",
    "group_plv_k_layers = np.mean(plv_layers_k,axis=3)\n",
    "group_iplv_k_layers = np.mean(iplv_layers_k,axis=3)\n",
    "\n",
    "group_plv_sur_layers = np.mean(plv_layers_sur, axis=3)\n",
    "group_iplv_sur_layers = np.mean(iplv_layers_sur, axis=3)\n",
    "\n",
    "plv_T_obs = np.squeeze(np.mean(np.abs(np.diff(plv_layers, axis=0)),axis=(3)))\n",
    "plv_pval = get_pvalue(plv_layers_stats, plv_T_obs)\n",
    "\n",
    "iplv_T_obs = np.squeeze(np.mean(np.abs(np.diff(iplv_layers, axis=0)),axis=(3)))\n",
    "iplv_pval = get_pvalue(iplv_layers_stats, iplv_T_obs)\n",
    "colors = np.array([[0, 146, 146], [146,0,0]])/255\n",
    "\n",
    "for bin_idx, ax_row in enumerate(axes.T):\n",
    "    plv_h = multipletests(plv_pval[bin_idx,:],alpha=.05, method='fdr_bh')[0]\n",
    "    ax_row[0].set_prop_cycle(color=colors)\n",
    "    ax_row[0].semilogx(frequencies, group_plv_layers[:,bin_idx,:].T, lw=4)\n",
    "    ax_row[0].fill_between(frequencies, *bootstrap_ci(plv_layers[0,bin_idx,:,:]), alpha=0.25)\n",
    "    ax_row[0].fill_between(frequencies, *bootstrap_ci(plv_layers[1,bin_idx,:,:]), alpha=0.25)\n",
    "    ax_row[0].semilogx(frequencies[plv_h], group_plv_layers[:,bin_idx,plv_h].T, 'o', markersize=10 )\n",
    "    ax_row[0].semilogx(frequencies, group_plv_sur_layers[:,bin_idx,:].T,'--' ,lw=1)\n",
    "    ax_row[0].set_ylim([0,.2])\n",
    "    axins = ax_row[0].inset_axes([0.48, 0.45, 0.4, 0.55])\n",
    "    axins.xaxis.set_major_formatter(ScalarFormatter())\n",
    "    axins.set_prop_cycle(color=colors)\n",
    "    axins.semilogx(frequencies[-18:], group_plv_layers[:,bin_idx,-18:].T, lw=4)\n",
    "    axins.fill_between(frequencies[-18:], *bootstrap_ci(plv_layers[0,bin_idx,-18:,:]), alpha=0.25)\n",
    "    axins.fill_between(frequencies[-18:], *bootstrap_ci(plv_layers[1,bin_idx,-18:,:]), alpha=0.25)\n",
    "\n",
    "    iplv_h = multipletests(iplv_pval[bin_idx,:],alpha=.05, method='fdr_bh')[0]\n",
    "    ax_row[1].set_prop_cycle(color=colors)\n",
    "    ax_row[1].semilogx(frequencies, group_iplv_layers[:,bin_idx,:].T, lw=4)\n",
    "    ax_row[1].fill_between(frequencies, *bootstrap_ci(iplv_layers[0,bin_idx,:,:]), alpha=0.25)\n",
    "    ax_row[1].fill_between(frequencies, *bootstrap_ci(iplv_layers[1,bin_idx,:,:]), alpha=0.25)\n",
    "    ax_row[1].semilogx(frequencies[iplv_h], group_iplv_layers[:,bin_idx,iplv_h].T, 'o', markersize=10)\n",
    "    ax_row[1].semilogx(frequencies, group_iplv_sur_layers[:,bin_idx,:].T,'--', lw=1)\n",
    "    ax_row[1].set_ylim([0, .08])\n",
    "    ax_row[1].set_ylim([0, .08])\n",
    "    axins = ax_row[1].inset_axes([0.48, 0.45, 0.4, 0.55])\n",
    "    axins.set_prop_cycle(color=colors)\n",
    "    axins.xaxis.set_major_formatter(ScalarFormatter())\n",
    "    axins.semilogx(frequencies[-18:], group_iplv_layers[:,bin_idx,-18:].T, lw=4)\n",
    "    axins.fill_between(frequencies[-18:], *bootstrap_ci(iplv_layers[0,bin_idx,-18:,:]), alpha=0.25)\n",
    "    axins.fill_between(frequencies[-18:], *bootstrap_ci(iplv_layers[1,bin_idx,-18:,:]), alpha=0.25)\n",
    "    \n",
    "    ax_row[2].set_prop_cycle(color=colors)\n",
    "    ax_row[2].semilogx(frequencies,group_plv_k_layers[:,bin_idx,:].T, lw=4)\n",
    "    ax_row[2].fill_between(frequencies, *bootstrap_ci(plv_layers_k[0,bin_idx,:,:]), alpha=0.25)\n",
    "    ax_row[2].fill_between(frequencies, *bootstrap_ci(plv_layers_k[1,bin_idx,:,:]), alpha=0.25)\n",
    "    ax_row[2].set_ylim([0, 1])\n",
    "\n",
    "    ax_row[3].set_prop_cycle(color=colors)\n",
    "    ax_row[3].semilogx(frequencies,group_iplv_k_layers[:,bin_idx,:].T, lw=4)\n",
    "    ax_row[3].fill_between(frequencies, *bootstrap_ci(iplv_layers_k[0,bin_idx,:,:]), alpha=0.25)\n",
    "    ax_row[3].fill_between(frequencies, *bootstrap_ci(iplv_layers_k[1,bin_idx,:,:]), alpha=0.25)\n",
    "    ax_row[3].set_ylim([0, 1])\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.tick_params(labelsize=18)\n",
    "    ax.set_xlabel('Frequency [Hz]', fontsize=24)\n",
    "    ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "    ax.set_xlim([frequencies.min(),frequencies.max()])\n",
    "    \n",
    "for ax, name in zip(axes[0], ['Very-short [<32 mm]', 'Short [32 < x <45]', 'Medium [45 < x < 60]', 'Long [60 < x < 137]']):\n",
    "    ax.set_title(name, fontsize=24)\n",
    "    ax.legend(('dd','ss'), loc='lower left')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('/home/gabri/localdata/dPLV/Fig5.pdf', format='pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split cohort reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = np.genfromtxt('/home/gabri/localdata/rest-bids/At_Least_1_Subj_per_Schaefer-Edge_BestSplit.csv', delimiter=',')\n",
    "n_splits, n_subjects = splits.shape\n",
    "\n",
    "idx_split = np.random.randint(0, 100, 1)   \n",
    "\n",
    "# for each splits get coh1 and coh2 subject indices\n",
    "coh1_indices = splits[idx_split,:32].astype(int).squeeze()\n",
    "coh2_indices = splits[idx_split,33:].astype(int).squeeze()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,4, figsize=(20,5), sharey='row')\n",
    "\n",
    "plv1 = np.nanmean(plv_layers[...,coh1_indices],axis=-1)\n",
    "plv2 = np.nanmean(plv_layers[...,coh2_indices],axis=-1)\n",
    "\n",
    "for idx, ax in enumerate(axs.T):\n",
    "    ax[0].set_prop_cycle(color=colors)\n",
    "    ax[1].set_prop_cycle(color=colors)\n",
    "    \n",
    "    ax[0].semilogx(frequencies, plv1[:,idx,:].T)\n",
    "    for jdx in range(2):\n",
    "        ax[0].fill_between(frequencies, *bootstrap_ci(plv_layers[jdx,idx,:,coh1_indices].T), alpha=.2)\n",
    "    ax[1].semilogx(frequencies, plv2[:,idx,:].T)\n",
    "    \n",
    "    for jdx in range(2):\n",
    "        ax[1].fill_between(frequencies, *bootstrap_ci(plv_layers[jdx,idx,:,coh2_indices].T), alpha=.2)\n",
    "        \n",
    "        \n",
    "for ax in axs.flatten():\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.set_xlabel('Frequency [Hz]', fontsize=8)\n",
    "    ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "    ax.set_xlim([frequencies.min(),frequencies.max()])\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig('/home/gabri/Fig.S5_split_layers.svg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
